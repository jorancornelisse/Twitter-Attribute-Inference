{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import textstat\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lit_emojis = [':(', ':-(', ':=(',':<', ':-<', ':)', ':-)', ':=)', ':D', ':-D', ':=D', ':d', ':-d', ':=d',\n",
    "              ':>', ':->', 'xd', ':o', ':-o', ':=o', ';)', ';-)', ';=)',\n",
    "              ';(', ';-(', ';=(', ':|', ':-|', ':=|',\n",
    "':P', ':-P', ':=P', ':p', ':-p', ':=p',\n",
    "':$', ':-$', ':=$',\n",
    "':]', ':-]',\n",
    "':*', ':-*',':=*',\n",
    "'|-)', 'I-)', 'I=)',\n",
    "'|-(', '|(', '|=( ',\n",
    "':&', ':-&', ':=&', '+o(',\n",
    "':@', ':-@', ':=@', 'x(', 'x-(', 'X(', 'X-(', 'x=(', 'X=(', ';@', ';-@',\n",
    "':S', ':s', ':-s', ':-S', ':=s', ':=S',\n",
    "              '8-|', 'B-|', '8|', 'B|', '8=|', 'B=|', \n",
    "':x', ':-x', ':#', ':-#', ':=x', ':=X', ':=#',\n",
    "':-?', ':?', ':=?', '*-)',\n",
    "':i', ':I', '*-:)']\n",
    "\n",
    "list_abbrs = ['2F4U',\n",
    "'4YEO',\n",
    "'FYEO',\n",
    "'AAMOF',\n",
    "'ACK',\n",
    "'AFAIK',\n",
    "'AFAIR',\n",
    "'AFK',\n",
    "'AKA',\n",
    "'B2K',\n",
    "'BTK',\n",
    "'BTT',\n",
    "'BTW',\n",
    "'B/C',\n",
    "'C&P',\n",
    "'CU',\n",
    "'CYS',\n",
    "'DIY',\n",
    "'EOBD',\n",
    "'EOD',\n",
    "'EOM',\n",
    "'EOT',\n",
    "'FAQ',\n",
    "'FACK',\n",
    "'FKA',\n",
    "'FWIW',\n",
    "'FYI',\n",
    "'JFYI',\n",
    "'FTW',\n",
    "'HF',\n",
    "'HTH',\n",
    "'IDK',\n",
    "'IIRC',\n",
    "'IMHO',\n",
    "'IMO',\n",
    "'IMNSHO',\n",
    "'IOW',\n",
    "'ITT',\n",
    "'LOL',\n",
    "'DGMW',\n",
    "'MMW',\n",
    "'N/A',\n",
    "'NaN',\n",
    "'NNTR',\n",
    "'noob',\n",
    "'n00b',\n",
    "'NOYB',\n",
    "'NRN',\n",
    "'OMG',\n",
    "'OP',\n",
    "'OT',\n",
    "'OTOH',\n",
    "'PEBKAC',\n",
    "'POV',\n",
    "'ROTFL',\n",
    "'RSVP',\n",
    "'RTFM',\n",
    "'SCNR',\n",
    "'SFLR',\n",
    "'SPOC',\n",
    "'TBA',\n",
    "'TBC',\n",
    "'TIA',\n",
    "'TGIF',\n",
    "'THX',\n",
    "'TNX',\n",
    "'TQ',\n",
    "'TYVM',\n",
    "'TYT',\n",
    "'TTYL',\n",
    "'w00t',\n",
    "'WFM',\n",
    "'WRT',\n",
    "'WTH',\n",
    "'WTF',\n",
    "'YMMD',\n",
    "'YMMV',\n",
    "'YAM',\n",
    "'ICYMI']\n",
    "\n",
    "swearwords_new = ['anus', 'arse',\n",
    "'arsehole',\n",
    "'ass',\n",
    "'ass-hat',\n",
    "'ass-jabber',\n",
    "'ass-pirate ',\n",
    "'assbag',\n",
    "'assbandit',\n",
    "'assbanger',\n",
    "'assbite',\n",
    "'assclown',\n",
    "'asscock',\n",
    "'asscracker',\n",
    "'asses',\n",
    "'assface',\n",
    "'assfuck',\n",
    "'assfucker',\n",
    "'assgoblin',\n",
    "'asshat',\n",
    "'asshead',\n",
    "'asshole',\n",
    "'asshopper',\n",
    "'assjacker',\n",
    "'asslick',\n",
    "'asslicker',\n",
    "'assmonkey',\n",
    "'assmunch',\n",
    "'assmuncher',\n",
    "'assnigger',\n",
    "'asspirate',\n",
    "'assshit',\n",
    "'assshole',\n",
    "'asssucker',\n",
    "'asswad',\n",
    "'asswipe',\n",
    "'axwound',\n",
    "'bampot',\n",
    "'bastard',\n",
    "'beaner',\n",
    "'bitch',\n",
    "'bitchass',\n",
    "'bitches',\n",
    "'bitchtits',\n",
    "'bitchy',\n",
    "'blow job',\n",
    "'blowjob',\n",
    "'bollocks',\n",
    "'bollox',\n",
    "'boner',\n",
    "'brotherfucker',\n",
    "'bullshit',\n",
    "'bumblefuck',\n",
    "'butt plug',\n",
    "'butt-pirate',\n",
    "'buttfucka',\n",
    "'buttfucker',\n",
    "'camel toe',\n",
    "'carpetmuncher',\n",
    "'chesticle',\n",
    "'chinc',\n",
    "'chink',\n",
    "'choad',\n",
    "'chode',\n",
    "'clit',\n",
    "'clitface',\n",
    "'clitfuck',\n",
    "'clusterfuck',\n",
    "'cock',\n",
    "'cockass',\n",
    "'cockbite',\n",
    "'cockburger',\n",
    "'cockface',\n",
    "'cockfucker',\n",
    "'cockhead',\n",
    "'cockjockey',\n",
    "'cockknoker',\n",
    "'cockmaster',\n",
    "'cockmongler',\n",
    "'cockmongruel', \n",
    "'cockmonkey',\n",
    "'cockmuncher',\n",
    "'cocknose',\n",
    "'cocknugget',\n",
    "'cockshit',\n",
    "'cocksmith',\n",
    "'cocksmoke',\n",
    "'cocksmoker',\n",
    "'cocksniffer',\n",
    "'cocksucker',\n",
    "'cockwaffle',\n",
    "'coochie',\n",
    "'coochy',\n",
    "'coon',\n",
    "'cooter',\n",
    "'cracker',\n",
    "'cum',\n",
    "'cumbubble',\n",
    "'cumdumpster',\n",
    "'cumguzzler',\n",
    "'cumjockey',\n",
    "'cumslut',\n",
    "'cumtart',\n",
    "'cunnie',\n",
    "'cunnilingus',\n",
    "'cunt',\n",
    "'cuntass',\n",
    "'cuntface',\n",
    "'cunthole',\n",
    "'cuntlicker',\n",
    "'cuntrag',\n",
    "'cuntslut',\n",
    "'dago',\n",
    "'damn',\n",
    "'deggo',\n",
    "'dick',\n",
    "'dick-sneeze',\n",
    "'dickbag',\n",
    "'dickbeaters',\n",
    "'dickface',\n",
    "'dickfuck',\n",
    "'dickfucker',\n",
    "'dickhead',\n",
    "'dickhole',\n",
    "'dickjuice',\n",
    "'dickmilk',\n",
    "'dickmonger',\n",
    "'dicks',\n",
    "'dickslap',\n",
    "'dicksucker',\n",
    "'dicksucking',\n",
    "'dicktickler',\n",
    "'dickwad',\n",
    "'dickweasel',\n",
    "'dickweed',\n",
    "'dickwod',\n",
    "'dike',\n",
    "'dildo',\n",
    "'dipshit',\n",
    "'doochbag',\n",
    "'dookie',\n",
    "'douche',\n",
    "'douche-fag',\n",
    "'douchebag',\n",
    "'douchewaffle',\n",
    "'dumass',\n",
    "'dumb ass',\n",
    "'dumbass',\n",
    "'dumbfuck',\n",
    "'dumbshit',\n",
    "'dumshit',\n",
    "'dyke',\n",
    "'fag',\n",
    "'fagbag',\n",
    "'fagfucker',\n",
    "'faggit',\n",
    "'faggot',\n",
    "'faggotcock',\n",
    "'fagtard',\n",
    "'fatass',\n",
    "'fellatio',\n",
    "'feltch',\n",
    "'flamer',\n",
    "'fuck',\n",
    "'fuckass',\n",
    "'fuckbag',\n",
    "'fuckboy',\n",
    "'fuckbrain',\n",
    "'fuckbutt',\n",
    "'fuckbutter',\n",
    "'fucked',\n",
    "'fucker',\n",
    "'fuckersucker',\n",
    "'fuckface',\n",
    "'fuckhead',\n",
    "'fuckhole',\n",
    "'fuckin',\n",
    "'fucking',\n",
    "'fucknut',\n",
    "'fucknutt',\n",
    "'fuckoff',\n",
    "'fucks',\n",
    "'fuckstick',\n",
    "'fucktard',\n",
    "'fucktart',\n",
    "'fuckup',\n",
    "'fuckwad',\n",
    "'fuckwit',\n",
    "'fuckwitt',\n",
    "'fudgepacker',\n",
    "'gay',\n",
    "'gayass',\n",
    "'gaybob',\n",
    "'gaydo',\n",
    "'gayfuck',\n",
    "'gayfuckist',\n",
    "'gaylord',\n",
    "'gaytard',\n",
    "'gaywad',\n",
    "'goddamn',\n",
    "'goddamnit',\n",
    "'gooch',\n",
    "'gook',\n",
    "'gringo',\n",
    "'guido',\n",
    "'handjob',\n",
    "'hard on',\n",
    "'heeb',\n",
    "'hell',\n",
    "'ho',\n",
    "'hoe',\n",
    "'homo',\n",
    "'homodumbshit',\n",
    "'honkey',\n",
    "'humping',\n",
    "'jackass',\n",
    "'jagoff',\n",
    "'jap',\n",
    "'jerk off',\n",
    "'jerkass',\n",
    "'jigaboo',\n",
    "'jizz',\n",
    "'jungle bunny',\n",
    "'junglebunny',\n",
    "'kike',\n",
    "'kooch',\n",
    "'kootch',\n",
    "'kraut',\n",
    "'kunt',\n",
    "'kyke',\n",
    "'lameass',\n",
    "'lardass',\n",
    "'lesbian',\n",
    "'lesbo',\n",
    "'lezzie',\n",
    "'mcfagget',\n",
    "'mick',\n",
    "'minge',\n",
    "'mothafucka',\n",
    "'mothafuckin',\n",
    "'motherfucker',\n",
    "'motherfucking',\n",
    "'muff',\n",
    "'muffdiver',\n",
    "'munging',\n",
    "'negro',\n",
    "'nigaboo',\n",
    "'nigga',\n",
    "'nigger',\n",
    "'niggers',\n",
    "'niglet',\n",
    "'nut sack',\n",
    "'nutsack',\n",
    "'paki',\n",
    "'panooch',\n",
    "'pecker',\n",
    "'peckerhead',\n",
    "'penis',\n",
    "'penisbanger',\n",
    "'penisfucker',\n",
    "'penispuffer',\n",
    "'piss',\n",
    "'pissed',\n",
    "'pissed off',\n",
    "'pissflaps',\n",
    "'polesmoker',\n",
    "'pollock',\n",
    "'poon',\n",
    "'poonani',\n",
    "'poonany',\n",
    "'poontang',\n",
    "'porch monkey',\n",
    "'porchmonkey',\n",
    "'prick',\n",
    "'punanny',\n",
    "'punta',\n",
    "'pussies',\n",
    "'pussy',\n",
    "'pussylicking',\n",
    "'puto',\n",
    "'queef',\n",
    "'queer',\n",
    "'queerbait',\n",
    "'queerhole',\n",
    "'renob',\n",
    "'rimjob',\n",
    "'ruski',\n",
    "'sand nigger',\n",
    "'sandnigger',\n",
    "'schlong',\n",
    "'scrote',\n",
    "'shit',\n",
    "'shitass',\n",
    "'shitbag',\n",
    "'shitbagger',\n",
    "'shitbrains',\n",
    "'shitbreath',\n",
    "'shitcanned',\n",
    "'shitcunt',\n",
    "'shitdick',\n",
    "'shitface',\n",
    "'shitfaced',\n",
    "'shithead',\n",
    "'shithole',\n",
    "'shithouse',\n",
    "'shitspitter',\n",
    "'shitstain',\n",
    "'shitter',\n",
    "'shittiest',\n",
    "'shitting',\n",
    "'shitty',\n",
    "'shiz',\n",
    "'shiznit',\n",
    "'skank',\n",
    "'skeet',\n",
    "'skullfuck',\n",
    "'slut',\n",
    "'slutbag',\n",
    "'smeg',\n",
    "'snatch',\n",
    "'spic',\n",
    "'spick',\n",
    "'splooge',\n",
    "'spook',\n",
    "'suckass',\n",
    "'tard',\n",
    "'testicle',\n",
    "'thundercunt',\n",
    "'tit',\n",
    "'titfuck',\n",
    "'tits',\n",
    "'tittyfuck',\n",
    "'twat',\n",
    "'twatlips',\n",
    "'twats',\n",
    "'twatwaffle',\n",
    "'unclefucker',\n",
    "'va-j-j',\n",
    "'vag',\n",
    "'vagina',\n",
    "'vajayjay',\n",
    "'vjayjay',\n",
    "'wank',\n",
    "'wankjob',\n",
    "'wetback',\n",
    "'whore',\n",
    "'whorebag',\n",
    "'whoreface',\n",
    "'wop']\n",
    "\n",
    "original_list = ['arse','ass','asshole','bastard','bitch','bollocks','bugger','child-fucker','Christ on a bike','Christ on a cracker',\n",
    " 'crap','cunt', 'cunts','damn','effing','frigger','fuck','goddamn','godsdamn','hell','holy shit','horseshit','Jesus Christ',\n",
    " 'Jesus fuck','Jesus H. Christ','Jesus Harold Christ','Jesus wept','Jesus, Mary and Joseph','Judas Priest',\n",
    " 'motherfucker','nigga','nigger','prick','shit','shit ass','shitass','slut','son of a bitch','son of a whore',\n",
    " 'sweet Jesus','twat']\n",
    "\n",
    "swearwords_data = pd.read_csv('swearWords.csv', header=None)\n",
    "second_list = list(swearwords_data.iloc[0,0:76])\n",
    "\n",
    "list_swearwords = swearwords_new + second_list + original_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_swearwords_lower = [swear.lower() for swear in list_swearwords]\n",
    "list_abbr_lower = [abbr.lower() for abbr in list_abbrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.read_csv('data_sample.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_new = data_new[[\n",
    "#     'AccountNaam',\n",
    "#  'AccountTweets',\n",
    "#  'AccountFollowers',\n",
    "#  'AccountFollowing',\n",
    "#  'AccountLocatie',\n",
    "#  'AccountSite',\n",
    "#  'AccountProfiel',\n",
    "#  'AccountCreatiedatum',\n",
    "#  'AccountTweetdatum',\n",
    "#  'Tweets',\n",
    "# 'Volgendaccount',\n",
    "# 'list_emojis']].sample(200).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_new.to_csv('data_sample.csv', index='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emojis = data_new['list_emojis'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_emojis = collections.Counter(all_emojis)\n",
    "most_common_emojis = counter_emojis.most_common(105)\n",
    "\n",
    "actual_most_common_emojis = most_common_emojis[3:6] + most_common_emojis[8:]\n",
    "list_top50_emojis = []\n",
    "for emoji in actual_most_common_emojis:\n",
    "    list_top50_emojis.append(emoji[0])\n",
    "# list_top50_emojis = list_top50_emojis[:3] + list_top50_emojis[7:]\n",
    "data_new['list_emojis'] = data_new['list_emojis'].fillna('')\n",
    "\n",
    "for i in range(len(list_top50_emojis)):\n",
    "    globals()[f'data_emoji{i}'] = []\n",
    "\n",
    "for emoji in data_new['list_emojis']:\n",
    "    for i in range(len(list_top50_emojis)):\n",
    "        result = emoji.count(list_top50_emojis[i])\n",
    "        globals()[f'data_emoji{i}'].append(result)\n",
    "        \n",
    "for i in range(len(list_top50_emojis)):\n",
    "    data_new[f'emoji{i}'] = globals()[f'data_emoji{i}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new['AccountProfiel'] = data_new['AccountProfiel'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_word(s, w):\n",
    "    return (' ' + w + ' ') in (' ' + s + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_students = ['retired', 'mom', 'dad', 'father', 'mother', 'kids',\n",
    "                'daugther', 'son', 'graduate', 'work', 'married',\n",
    "                'partner', 'ago', 'more than', 'looks', 'grandparent'] + [str(i) for i in range(12, 18)] + [str(i) for i in range(25, 100)]\n",
    "\n",
    "bios = list(data_new['AccountProfiel'])\n",
    "lst_bios = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    output = any(word in bio for word in remove_students)\n",
    "    output_reverse = output == False\n",
    "    lst_bios.append(output_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_students = data_new[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" years old\"\n",
    "words = [str(i)+string for i in range(18, 25)]\n",
    "\n",
    "bios = list(data_students['AccountProfiel'])\n",
    "lst_bios = []\n",
    "ages = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    lst_bools = []\n",
    "    age = re.search(\"\\d+\", bio)\n",
    "    for word in words:\n",
    "        boolean = contains_word(bio, word)\n",
    "        lst_bools.append(boolean)\n",
    "    if any(lst_bools) == True:\n",
    "        output = True\n",
    "    else:\n",
    "        output = False\n",
    "        ages.append(0)\n",
    "        \n",
    "    lst_bios.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_students = data_students[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_adults = ['retired', 'grandparent'] + [str(i) for i in range(12, 25)] + [str(i) for i in range(55, 100)]\n",
    "\n",
    "bios = list(data_new['AccountProfiel'])\n",
    "lst_bios = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    output = any(word in bio for word in remove_adults)\n",
    "    output_reverse = output == False\n",
    "    lst_bios.append(output_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adults = data_new[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" years old\"\n",
    "words = [str(i)+string for i in range(25, 55)]\n",
    "\n",
    "bios = list(data_adults['AccountProfiel'])\n",
    "lst_bios = []\n",
    "ages = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    lst_bools = []\n",
    "    age = re.search(\"\\d+\", bio)\n",
    "    for word in words:\n",
    "        boolean = contains_word(bio, word)\n",
    "        lst_bools.append(boolean)\n",
    "    if any(lst_bools) == True:\n",
    "        output = True\n",
    "#         ages.append(words[lst_bools])\n",
    "    else:\n",
    "        output = False\n",
    "        ages.append(0)\n",
    "        \n",
    "    lst_bios.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adults = data_adults[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_retired = [str(i) for i in range(12, 55)] + [str(i) for i in range(70, 100)]\n",
    "\n",
    "bios = list(data_new['AccountProfiel'])\n",
    "lst_bios = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    output = any(word in bio for word in remove_retired)\n",
    "    output_reverse = output == False\n",
    "    lst_bios.append(output_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_retired = data_new[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" years old\"\n",
    "words = [str(i)+string for i in range(55, 70)]\n",
    "\n",
    "bios = list(data_retired['AccountProfiel'])\n",
    "lst_bios = []\n",
    "ages = []\n",
    "for bio in bios:\n",
    "    bio = bio.lower()\n",
    "    lst_bools = []\n",
    "    age = re.search(\"\\d+\", bio)\n",
    "    for word in words:\n",
    "        boolean = contains_word(bio, word)\n",
    "        lst_bools.append(boolean)\n",
    "    if any(lst_bools) == True:\n",
    "        output = True\n",
    "#         ages.append(words[lst_bools])\n",
    "    else:\n",
    "        output = False\n",
    "        ages.append(0)\n",
    "        \n",
    "    lst_bios.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_retired = data_retired[lst_bios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_students['Class'] = 0\n",
    "data_adults['Class'] = 1\n",
    "data_retired['Class'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_students, data_adults, data_retired]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all = data[['AccountTweets', 'AccountFollowers',\n",
    "#        'AccountFollowing', 'AccountLocatie', 'AccountSite', 'AccountProfiel',\n",
    "#        'AccountCreatiedatum', 'AccountTweetdatum', 'Tweets', 'Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punct = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "all_tweets = []\n",
    "rt_count = []\n",
    "\n",
    "for tweets in data['Tweets']:\n",
    "    rt = 0\n",
    "    tweets_user = []\n",
    "    for i in range(len(tweets.split(\"\\'\"))):\n",
    "        tweet_fragment = tweets.split(\"\\'\")[i]\n",
    "        if tweet_fragment.count('RT') != 0:\n",
    "            rt = rt + 1\n",
    "        else:\n",
    "            tweets_user.append(tweet_fragment)\n",
    "    rt_count.append(rt)\n",
    "    all_tweets.append(' '.join(tweets_user))\n",
    "    \n",
    "    if len(rt_count) % 1000 == 0:\n",
    "        print(len(rt_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "import string\n",
    "\n",
    "count_punct = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "def extract_emojis(a_list):\n",
    "    emojis_list = map(lambda x: ''.join(x.split()), emoji.UNICODE_EMOJI.keys())\n",
    "    r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "    aux=[' '.join(r.findall(s)) for s in a_list]\n",
    "    return(aux)\n",
    "\n",
    "urls_count = []\n",
    "hashtag_count = []\n",
    "capital_count = []\n",
    "tweet_length = []\n",
    "punctuation_count = []\n",
    "lit_emojis_count = []\n",
    "list_emojis = []\n",
    "emoji_count = []\n",
    "\n",
    "for tweet in all_tweets:\n",
    "    urls_count.append(tweet.count('http'))\n",
    "    hashtag_count.append(tweet.count('#'))\n",
    "    capital_count.append(sum(1 for c in tweet if c.isupper()))\n",
    "    tweet_length.append(len(tweet))\n",
    "    punctuation_count.append(count_punct(tweet, string.punctuation))\n",
    "    lit_emojis_count.append(sum(tweet.count(x) for x in list_lit_emojis))\n",
    "    \n",
    "    emojis_user = [s for s in extract_emojis(tweet) if s != '']\n",
    "    list_emojis.append(emojis_user)\n",
    "    emoji_count.append(len(emojis_user))\n",
    "    \n",
    "    if len(urls_count) % 1000 == 0:\n",
    "        print(len(urls_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rt_count'] = rt_count\n",
    "\n",
    "data['urls_count'] = urls_count\n",
    "data['hashtag_count'] = hashtag_count\n",
    "data['capital_count'] = capital_count\n",
    "data['tweet_length'] = tweet_length\n",
    "data['punctuation_count'] = punctuation_count\n",
    "data['list_emojis'] = list_emojis\n",
    "data['emoji_count'] = emoji_count\n",
    "\n",
    "data['literal_emojis'] = lit_emojis_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_abbr_lower = [abbr.lower() for abbr in list_abbrs]\n",
    "\n",
    "laugh_count = []\n",
    "diff_words_count = []\n",
    "\n",
    "clean_tweets = []\n",
    "\n",
    "for tweet in all_tweets:\n",
    "    \n",
    "    tweet_no_link = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet_stripped = tweet_no_link.lower().translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    clean_tweet = re.sub(' +', ' ', tweet_stripped)\n",
    "    \n",
    "    laugh_count.append(clean_tweet.count('hah'))\n",
    "    diff_words_count.append(textstat.difficult_words(clean_tweet))\n",
    "    \n",
    "    clean_tweets.append(clean_tweet)\n",
    "    \n",
    "    if len(clean_tweets) % 1000 == 0:\n",
    "        print(len(clean_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['laugh_count'] = laugh_count\n",
    "data['diff_words_count'] = diff_words_count\n",
    "data['clean_tweets'] = clean_tweets\n",
    "\n",
    "# data_all['swear_count'] = swear_count\n",
    "# data_all['abbr_count'] = abbr_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin13_18 = pd.read_csv('age_bins.13_18.gender_adjusted.rmatrix.top100s.csv')\n",
    "bin13_18_neg = list(bin13_18.iloc[2:,0])\n",
    "bin13_18_pos = list(bin13_18.iloc[2:,3])\n",
    "\n",
    "bin19_22 = pd.read_csv('age_bins.19_22.gender_adjusted.rmatrix.top100s.csv')\n",
    "bin19_22_neg = list(bin19_22.iloc[2:,0])\n",
    "bin19_22_pos = list(bin19_22.iloc[2:,3])\n",
    "\n",
    "bin23_29 = pd.read_csv('age_bins.23_29.gender_adjusted.rmatrix.top100s.csv')\n",
    "bin23_29_neg = list(bin23_29.iloc[2:,0])\n",
    "bin23_29_pos = list(bin23_29.iloc[2:,3])\n",
    "\n",
    "bin30 = pd.read_csv('age_bins.30_+.gender_adjusted.rmatrix.top100s.csv')\n",
    "bin30_neg = list(bin30.iloc[2:,0])\n",
    "bin30_pos = list(bin30.iloc[2:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_gender = pd.read_excel('gender.top100.1to3grams_cleaned.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_women = list(bins_gender.iloc[:,0])\n",
    "bins_men = list(bins_gender.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_men_clean = []\n",
    "for i in bins_men:\n",
    "    bins_men_clean.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweets'] = clean_tweets\n",
    "data['clean_tweets'] = data['clean_tweets'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bins_women = []\n",
    "count_bins_men = []\n",
    "\n",
    "\n",
    "for tweet in data['clean_tweets']:\n",
    "    \n",
    "    count_bins_women.append(sum(tweet.count(x) for x in bins_women))\n",
    "    count_bins_men.append(sum(tweet.count(x) for x in bins_men_clean))\n",
    "    \n",
    "    \n",
    "    if len(count_bins_men) % 100 == 0:\n",
    "        print(len(count_bins_men))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = []\n",
    "for i in count_bins_men:\n",
    "    if i == 0:\n",
    "        bools.append(True)\n",
    "    else:\n",
    "        bools.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_count = []\n",
    "swear_count = []\n",
    "\n",
    "count_13_18_pos = []\n",
    "count_13_18_neg = []\n",
    "\n",
    "count_19_22_pos = []\n",
    "count_19_22_neg = []\n",
    "\n",
    "count_23_29_pos = []\n",
    "count_23_29_neg = []\n",
    "\n",
    "count_30_pos = []\n",
    "count_30_neg = []\n",
    "\n",
    "\n",
    "for tweet in clean_tweets:\n",
    "    \n",
    "    swear_count.append(sum(tweet.count(x) for x in list_swearwords_lower))\n",
    "    abbr_count.append(sum(tweet.count(x) for x in list_abbr_lower))\n",
    "    \n",
    "    count_13_18_pos.append(sum(tweet.count(x) for x in bin13_18_pos))\n",
    "    count_13_18_neg.append(sum(tweet.count(x) for x in bin13_18_neg))\n",
    "    \n",
    "    count_19_22_pos.append(sum(tweet.count(x) for x in bin19_22_pos))\n",
    "    count_19_22_neg.append(sum(tweet.count(x) for x in bin19_22_neg))\n",
    "    \n",
    "    count_23_29_pos.append(sum(tweet.count(x) for x in bin23_29_pos))\n",
    "    count_23_29_neg.append(sum(tweet.count(x) for x in bin23_29_neg))\n",
    "    \n",
    "    count_30_pos.append(sum(tweet.count(x) for x in bin30_pos))\n",
    "    count_30_neg.append(sum(tweet.count(x) for x in bin30_neg))\n",
    "    \n",
    "    \n",
    "    if len(count_30_neg) % 100 == 0:\n",
    "        print(len(count_30_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['abbr_count'] = abbr_count\n",
    "data['swear_count'] = swear_count\n",
    "data['count_13_18_pos'] = count_13_18_pos\n",
    "data['count_13_18_neg'] = count_13_18_neg\n",
    "data['count_19_22_pos'] = count_19_22_pos\n",
    "data['count_19_22_neg'] = count_19_22_neg\n",
    "data['count_23_29_pos'] = count_23_29_pos\n",
    "data['count_23_29_neg'] = count_23_29_neg\n",
    "data['count_30_pos'] = count_30_pos\n",
    "data['count_30_neg'] = count_30_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitted_tweets = []\n",
    "# for tweet in clean_tweets:\n",
    "#     splitted_tweets.append(tweet.split(' '))\n",
    "#     print(len(splitted_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nnp = []\n",
    "count_nn = []\n",
    "count_rb = []\n",
    "count_jj = []\n",
    "count_nns = []\n",
    "count_in = []\n",
    "count_fw = []\n",
    "count_nnps = []\n",
    "count_vbp = []\n",
    "count_cd = []\n",
    "count_vbd = []\n",
    "count_md = []\n",
    "count_vb = []\n",
    "count_vbg = []\n",
    "count_vbz = []\n",
    "count_rbr = []\n",
    "count_vbn = []\n",
    "count_jjs = []\n",
    "count_cc = []\n",
    "\n",
    "tweets_bools = []\n",
    "\n",
    "for whole_tweet in clean_tweets:\n",
    "    tweet = whole_tweet.split(' ')\n",
    "    if len(tweet) > 10:\n",
    "        tweets_bools.append(True)\n",
    "        count= Counter([j for i,j in pos_tag(tweet)])\n",
    "        try:\n",
    "            count_nnp.append(count['NNP'])\n",
    "        except:\n",
    "            count_nnp.append(0)\n",
    "\n",
    "        try:\n",
    "            count_nn.append(count['NN'])\n",
    "        except:\n",
    "            count_nn.append(0)\n",
    "\n",
    "        try:\n",
    "            count_rb.append(count['RB'])\n",
    "        except:\n",
    "            count_rb.append(0)\n",
    "\n",
    "        try:\n",
    "            count_jj.append(count['JJ'])\n",
    "        except:\n",
    "            count_jj.append(0)\n",
    "\n",
    "        try:\n",
    "            count_nns.append(count['NNS'])\n",
    "        except:\n",
    "            count_nns.append(0)\n",
    "\n",
    "        try:\n",
    "            count_in.append(count['IN'])\n",
    "        except:\n",
    "            count_in.append(0)\n",
    "\n",
    "        try:\n",
    "            count_fw.append(count['FW'])\n",
    "        except:\n",
    "            count_fw.append(0)\n",
    "\n",
    "        try:\n",
    "            count_nnps.append(count['NNPS'])\n",
    "        except:\n",
    "            count_nnps.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vbp.append(count['VBP'])\n",
    "        except:\n",
    "            count_vbp.append(0)\n",
    "\n",
    "        try:\n",
    "            count_cd.append(count['CD'])\n",
    "        except:\n",
    "            count_cd.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vbd.append(count['VBD'])\n",
    "        except:\n",
    "            count_vbd.append(0)\n",
    "\n",
    "        try:\n",
    "            count_md.append(count['MD'])\n",
    "        except:\n",
    "            count_md.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vb.append(count['VB'])\n",
    "        except:\n",
    "            count_vb.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vbg.append(count['VBG'])\n",
    "        except:\n",
    "            count_vbg.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vbz.append(count['VBZ'])\n",
    "        except:\n",
    "            count_vbz.append(0)\n",
    "\n",
    "        try:\n",
    "            count_rbr.append(count['RBR'])\n",
    "        except:\n",
    "            count_rbr.append(0)\n",
    "\n",
    "        try:\n",
    "            count_vbn.append(count['VBN'])\n",
    "        except:\n",
    "            count_vbn.append(0)\n",
    "\n",
    "        try:\n",
    "            count_jjs.append(count['JJS'])\n",
    "        except:\n",
    "            count_jjs.append(0)\n",
    "\n",
    "        try:\n",
    "            count_cc.append(count['CC'])\n",
    "        except:\n",
    "            count_cc.append(0)\n",
    "    else:\n",
    "        tweets_bools.append(False)\n",
    "    \n",
    "    if len(tweets_bools) % 1000 == 0:\n",
    "        print(len(tweets_bools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[tweets_bools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nnp'] = count_nnp\n",
    "data['nn'] = count_nn\n",
    "data['rb'] = count_rb\n",
    "data['in'] = count_in\n",
    "data['fw'] = count_fw\n",
    "data['nnps'] = count_nnps\n",
    "data['vbp'] = count_vbp\n",
    "data['cd'] = count_cd\n",
    "data['vbd'] = count_vbd\n",
    "data['md'] = count_md\n",
    "data['vb'] = count_vb\n",
    "data['vbg'] = count_vbg\n",
    "data['vbz'] = count_vbz\n",
    "data['rbr'] = count_rbr\n",
    "data['vbn'] = count_vbn\n",
    "data['jjs'] = count_jjs\n",
    "data['cc'] = count_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "dict_month = {v: k for k,v in enumerate(calendar.month_abbr)}\n",
    "\n",
    "dates = []\n",
    "true_instances = []\n",
    "for date in data['AccountCreatiedatum']:\n",
    "    try:\n",
    "        splitted = date.split()\n",
    "        dates.append(str(splitted[5])+'-'+str(dict_month[splitted[1]])+'-'+str(splitted[2]))\n",
    "        true_instances.append(True)\n",
    "    except:\n",
    "        true_instances.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data[true_instances].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "days_creation_dates = []\n",
    "for creation_date in dates:\n",
    "    checkdate = datetime.datetime.strptime(str(creation_date), \"%Y-%m-%d\")\n",
    "    days = (datetime.datetime.now()-checkdate).days\n",
    "    days_creation_dates.append(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new['creation_date'] = days_creation_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "import re\n",
    "for bio in data_all['AccountProfiel']:\n",
    "    age = re.search(\"\\d+\", bio)\n",
    "    try:\n",
    "        ages.append(age.group(0))\n",
    "    except:\n",
    "        ages.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['Age'] = ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sites = []\n",
    "for i in data_all['AccountSite']:\n",
    "    if pd.isnull(i):\n",
    "        sites.append(0)\n",
    "    else:\n",
    "        sites.append(1)\n",
    "\n",
    "len_bios = []\n",
    "for i in data_all['AccountProfiel']:\n",
    "    len_bios.append(len(i))\n",
    "    \n",
    "array_bio = np.array(len_bios)\n",
    "normalized_bio = preprocessing.normalize([array_bio])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sites)\n",
    "\n",
    "sites_encoded = le.transform(sites)\n",
    "\n",
    "data_all['length_bio'] = array_bio\n",
    "data_all['site'] = sites_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
